{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1: Converting TXT files to XLSX...\n",
      "Converted: /Users/brenthokeness/Desktop/BiomechData/Fall2024Baseball-VJTesting/BBVJAPR16-TXT/CAV_4-16-25_VJ_T2 - PowerVJ.txt -> /Users/brenthokeness/Desktop/BiomechData/Fall2024Baseball-VJTesting/BBVJAPR16-INT/CAV_4-16-25_VJ_T2 - PowerVJ.xlsx\n",
      "Converted: /Users/brenthokeness/Desktop/BiomechData/Fall2024Baseball-VJTesting/BBVJAPR16-TXT/JTH_4-16-25_VJ_T1 - PowerVJ.txt -> /Users/brenthokeness/Desktop/BiomechData/Fall2024Baseball-VJTesting/BBVJAPR16-INT/JTH_4-16-25_VJ_T1 - PowerVJ.xlsx\n",
      "Converted: /Users/brenthokeness/Desktop/BiomechData/Fall2024Baseball-VJTesting/BBVJAPR16-TXT/LRB_4-16-25_VJ_T2 - PowerVJ.txt -> /Users/brenthokeness/Desktop/BiomechData/Fall2024Baseball-VJTesting/BBVJAPR16-INT/LRB_4-16-25_VJ_T2 - PowerVJ.xlsx\n",
      "Converted: /Users/brenthokeness/Desktop/BiomechData/Fall2024Baseball-VJTesting/BBVJAPR16-TXT/JTH_4-16-25_VJ_T2 - PowerVJ.txt -> /Users/brenthokeness/Desktop/BiomechData/Fall2024Baseball-VJTesting/BBVJAPR16-INT/JTH_4-16-25_VJ_T2 - PowerVJ.xlsx\n",
      "Converted: /Users/brenthokeness/Desktop/BiomechData/Fall2024Baseball-VJTesting/BBVJAPR16-TXT/LRB_4-16-25_VJ_T1 - PowerVJ.txt -> /Users/brenthokeness/Desktop/BiomechData/Fall2024Baseball-VJTesting/BBVJAPR16-INT/LRB_4-16-25_VJ_T1 - PowerVJ.xlsx\n",
      "Converted: /Users/brenthokeness/Desktop/BiomechData/Fall2024Baseball-VJTesting/BBVJAPR16-TXT/CAV_4-16-25_VJ_T1 - PowerVJ.txt -> /Users/brenthokeness/Desktop/BiomechData/Fall2024Baseball-VJTesting/BBVJAPR16-INT/CAV_4-16-25_VJ_T1 - PowerVJ.xlsx\n",
      "Converted: /Users/brenthokeness/Desktop/BiomechData/Fall2024Baseball-VJTesting/BBVJAPR16-TXT/CTZ_4-16-25_VJ_T2 - PowerVJ.txt -> /Users/brenthokeness/Desktop/BiomechData/Fall2024Baseball-VJTesting/BBVJAPR16-INT/CTZ_4-16-25_VJ_T2 - PowerVJ.xlsx\n",
      "Converted: /Users/brenthokeness/Desktop/BiomechData/Fall2024Baseball-VJTesting/BBVJAPR16-TXT/KJK_4-16-25_VJ_T2 - PowerVJ.txt -> /Users/brenthokeness/Desktop/BiomechData/Fall2024Baseball-VJTesting/BBVJAPR16-INT/KJK_4-16-25_VJ_T2 - PowerVJ.xlsx\n",
      "Converted: /Users/brenthokeness/Desktop/BiomechData/Fall2024Baseball-VJTesting/BBVJAPR16-TXT/KJK_4-16-25_VJ_T1 - PowerVJ.txt -> /Users/brenthokeness/Desktop/BiomechData/Fall2024Baseball-VJTesting/BBVJAPR16-INT/KJK_4-16-25_VJ_T1 - PowerVJ.xlsx\n",
      "Converted: /Users/brenthokeness/Desktop/BiomechData/Fall2024Baseball-VJTesting/BBVJAPR16-TXT/CTZ_4-16-25_VJ_T1 - PowerVJ.txt -> /Users/brenthokeness/Desktop/BiomechData/Fall2024Baseball-VJTesting/BBVJAPR16-INT/CTZ_4-16-25_VJ_T1 - PowerVJ.xlsx\n",
      "Converted: /Users/brenthokeness/Desktop/BiomechData/Fall2024Baseball-VJTesting/BBVJAPR16-TXT/CMR_4-16-25_VJ_T2 - PowerVJ.txt -> /Users/brenthokeness/Desktop/BiomechData/Fall2024Baseball-VJTesting/BBVJAPR16-INT/CMR_4-16-25_VJ_T2 - PowerVJ.xlsx\n",
      "Converted: /Users/brenthokeness/Desktop/BiomechData/Fall2024Baseball-VJTesting/BBVJAPR16-TXT/AJR_4-16-25_VJ_T1 - PowerVJ.txt -> /Users/brenthokeness/Desktop/BiomechData/Fall2024Baseball-VJTesting/BBVJAPR16-INT/AJR_4-16-25_VJ_T1 - PowerVJ.xlsx\n",
      "Converted: /Users/brenthokeness/Desktop/BiomechData/Fall2024Baseball-VJTesting/BBVJAPR16-TXT/CMR_4-16-25_VJ_T1 - PowerVJ.txt -> /Users/brenthokeness/Desktop/BiomechData/Fall2024Baseball-VJTesting/BBVJAPR16-INT/CMR_4-16-25_VJ_T1 - PowerVJ.xlsx\n",
      "Converted: /Users/brenthokeness/Desktop/BiomechData/Fall2024Baseball-VJTesting/BBVJAPR16-TXT/AJR_4-16-25_VJ_T2 - PowerVJ.txt -> /Users/brenthokeness/Desktop/BiomechData/Fall2024Baseball-VJTesting/BBVJAPR16-INT/AJR_4-16-25_VJ_T2 - PowerVJ.xlsx\n",
      "Converted: /Users/brenthokeness/Desktop/BiomechData/Fall2024Baseball-VJTesting/BBVJAPR16-TXT/EJP_4-16-25_VJ_T2 - PowerVJ.txt -> /Users/brenthokeness/Desktop/BiomechData/Fall2024Baseball-VJTesting/BBVJAPR16-INT/EJP_4-16-25_VJ_T2 - PowerVJ.xlsx\n",
      "Converted: /Users/brenthokeness/Desktop/BiomechData/Fall2024Baseball-VJTesting/BBVJAPR16-TXT/DCS_4-16-25_VJ_T1 - PowerVJ.txt -> /Users/brenthokeness/Desktop/BiomechData/Fall2024Baseball-VJTesting/BBVJAPR16-INT/DCS_4-16-25_VJ_T1 - PowerVJ.xlsx\n",
      "Converted: /Users/brenthokeness/Desktop/BiomechData/Fall2024Baseball-VJTesting/BBVJAPR16-TXT/DCS_4-16-25_VJ_T2 - PowerVJ.txt -> /Users/brenthokeness/Desktop/BiomechData/Fall2024Baseball-VJTesting/BBVJAPR16-INT/DCS_4-16-25_VJ_T2 - PowerVJ.xlsx\n",
      "Converted: /Users/brenthokeness/Desktop/BiomechData/Fall2024Baseball-VJTesting/BBVJAPR16-TXT/EJP_4-16-25_VJ_T1 - PowerVJ.txt -> /Users/brenthokeness/Desktop/BiomechData/Fall2024Baseball-VJTesting/BBVJAPR16-INT/EJP_4-16-25_VJ_T1 - PowerVJ.xlsx\n",
      "\n",
      "Stage 2: Processing XLSX files...\n",
      "\n",
      "Processing Complete:\n",
      "Files processed: 18\n",
      "Files with errors: 0\n",
      "\n",
      "Stage 3: Running Force Data Analysis on Processed Files...\n",
      "\n",
      "Analysis report generated and saved to /Users/brenthokeness/Desktop/BiomechData/Fall2024Baseball-VJTesting/BBVJAPR16_analysis_report_file.xlsx\n",
      "\n",
      "Complete pipeline finished!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "import glob\n",
    "import numpy as np\n",
    "import warnings\n",
    "from zipfile import BadZipFile\n",
    "\n",
    "# ------------------- Configuration Section -------------------\n",
    "# Define a unified base folder. All other folders are defined relative to this.\n",
    "BASE_FOLDER = \"/Users/brenthokeness/Desktop/BiomechData/Fall2024Baseball-VJTesting\"\n",
    "\n",
    "# Folder containing the raw TXT files\n",
    "TXT_INPUT_FOLDER = os.path.join(BASE_FOLDER, \"BBVJAPR16-TXT\")\n",
    "\n",
    "# Folder to store the intermediate XLSX files produced from TXT conversion\n",
    "XLSX_INTERMEDIATE_FOLDER = os.path.join(BASE_FOLDER, \"BBVJAPR16-INT\")\n",
    "\n",
    "# Folder to store the final processed XLSX files\n",
    "PROCESSED_XLSX_FOLDER = os.path.join(BASE_FOLDER, \"BBVJAPR16-PROCCESED\")\n",
    "\n",
    "# Full path (including filename) for the analysis report output\n",
    "ANALYSIS_REPORT_FILE = os.path.join(BASE_FOLDER, \"BBVJAPR16_analysis_report_file.xlsx\")\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "# Ignore FutureWarnings for cleaner output\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# ---------- Functions for TXT-to-XLSX conversion --------------\n",
    "def detect_delimiter(txt_file_path):\n",
    "    with open(txt_file_path, 'r') as file:\n",
    "        first_line = file.readline()\n",
    "        # Check common delimiters: comma, tab, or space\n",
    "        if ',' in first_line:\n",
    "            return ','\n",
    "        elif '\\t' in first_line:\n",
    "            return '\\t'\n",
    "        elif ' ' in first_line:\n",
    "            return ' '\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "def convert_txt_to_xlsx(txt_file_path, output_folder):\n",
    "    try:\n",
    "        # Read the header lines (assumed to be the first 5 lines)\n",
    "        header_lines = []\n",
    "        with open(txt_file_path, 'r') as file:\n",
    "            for i in range(5):  # Adjust if needed\n",
    "                header_lines.append(file.readline().strip())\n",
    "        \n",
    "        # Determine the delimiter\n",
    "        delimiter = detect_delimiter(txt_file_path)\n",
    "        if not delimiter:\n",
    "            raise ValueError(f\"Unknown delimiter in file: {txt_file_path}\")\n",
    "\n",
    "        # Read the data portion (skipping the header lines)\n",
    "        df = pd.read_csv(txt_file_path, delimiter=delimiter, skiprows=len(header_lines),\n",
    "                         engine=\"python\", quoting=csv.QUOTE_NONE)\n",
    "        \n",
    "        # Prepare the output file path (change .txt to .xlsx)\n",
    "        base_name = os.path.basename(txt_file_path).replace('.txt', '.xlsx')\n",
    "        xlsx_file_path = os.path.join(output_folder, base_name)\n",
    "        \n",
    "        # Write both the header (as text) and the data to the Excel file\n",
    "        with pd.ExcelWriter(xlsx_file_path, engine='openpyxl') as writer:\n",
    "            # Write header lines to the top of the sheet\n",
    "            header_df = pd.DataFrame(header_lines)\n",
    "            header_df.to_excel(writer, sheet_name='Sheet1', index=False, header=False)\n",
    "            # Write the main data starting below the header (offset by header lines + 2)\n",
    "            df.to_excel(writer, sheet_name='Sheet1', index=False, startrow=len(header_lines) + 2)\n",
    "        \n",
    "        print(f\"Converted: {txt_file_path} -> {xlsx_file_path}\")\n",
    "    except pd.errors.ParserError as e:\n",
    "        print(f\"ParserError while reading {txt_file_path}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting {txt_file_path}: {e}\")\n",
    "\n",
    "def batch_convert_txt_to_xlsx(input_folder, output_folder):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    for file_name in os.listdir(input_folder):\n",
    "        if file_name.endswith('.txt'):\n",
    "            txt_file_path = os.path.join(input_folder, file_name)\n",
    "            convert_txt_to_xlsx(txt_file_path, output_folder)\n",
    "\n",
    "# ------------- Functions for Processing XLSX Files --------------\n",
    "def validate_excel_file(file_path):\n",
    "    try:\n",
    "        pd.read_excel(file_path, nrows=1, engine='openpyxl')\n",
    "        return True\n",
    "    except (BadZipFile, Exception):\n",
    "        return False\n",
    "\n",
    "def find_threshold_crossing(df, start_index, threshold, direction, filename):\n",
    "    # Depending on direction, reverse the slice or not\n",
    "    if direction == \"backward\":\n",
    "        df_slice = df.loc[:start_index].iloc[::-1]\n",
    "    else:\n",
    "        df_slice = df.loc[start_index:]\n",
    "    \n",
    "    prev_value = df_slice.iloc[0]['Total_GRF']\n",
    "    crossing_count = 0\n",
    "    for index, row in df_slice.iterrows():\n",
    "        current_value = row['Total_GRF']\n",
    "        if (prev_value < threshold and current_value >= threshold) or \\\n",
    "           (prev_value > threshold and current_value <= threshold):\n",
    "            crossing_count += 1\n",
    "            if crossing_count == 3:\n",
    "                return index\n",
    "        prev_value = current_value\n",
    "    if crossing_count == 2:\n",
    "        return index\n",
    "    return None\n",
    "\n",
    "def process_files(input_folder, output_folder):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    excel_files = glob.glob(os.path.join(input_folder, '*.xlsx'))\n",
    "    processed_count = 0\n",
    "    error_count = 0\n",
    "    \n",
    "    for file_path in excel_files:\n",
    "        filename = os.path.basename(file_path)\n",
    "        \n",
    "        if not validate_excel_file(file_path):\n",
    "            print(f\"Skipping corrupted file: {filename}\")\n",
    "            error_count += 1\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # Read the converted file; note the header is in row 8 (index 7)\n",
    "            df = pd.read_excel(file_path, header=7, engine='openpyxl')\n",
    "            \n",
    "            # Check for required columns in the data (as available in the TXT file)\n",
    "            if \"Sample #\" not in df.columns or \"Total_GRF\" not in df.columns or \"PeakJumpHeight\" not in df.columns:\n",
    "                print(f\"Missing required columns in {filename}\")\n",
    "                error_count += 1\n",
    "                continue\n",
    "            \n",
    "            # Initialize new columns\n",
    "            df[\"Total_GRF\"] = df[\"Total_GRF\"].abs()\n",
    "            df[\"VEM1\"] = 0\n",
    "            df[\"VEM2\"] = 0\n",
    "            df[\"VEM3\"] = 0\n",
    "            df[\"VEM4\"] = 0\n",
    "            df[\"VEM5\"] = 0\n",
    "            df[\"Power\"] = 0\n",
    "            df[\"Time\"] = df[\"Sample #\"] / 238.095  # Convert sample number to time\n",
    "            \n",
    "            # Mark VEM2 (takeoff) and VEM3 (landing)\n",
    "            takeoff_idx = df[df[\"Total_GRF\"] < 20].index.min()\n",
    "            if pd.notna(takeoff_idx):\n",
    "                df.at[takeoff_idx, \"VEM2\"] = 1\n",
    "                weight_acceptance_df = df.loc[takeoff_idx + 1:]\n",
    "                weight_acceptance_idx = weight_acceptance_df[weight_acceptance_df[\"Total_GRF\"] > 20].index.min()\n",
    "                if pd.notna(weight_acceptance_idx):\n",
    "                    df.at[weight_acceptance_idx, \"VEM3\"] = 1\n",
    "            \n",
    "            # Calculate a baseline and threshold (95% of the baseline)\n",
    "            baseline = df['Total_GRF'].iloc[:10].mean()\n",
    "            threshold = baseline * 0.95\n",
    "\n",
    "            # Find VEM1 (backward crossing) relative to VEM2\n",
    "            vem2_indices = df[df['VEM2'] == 1].index\n",
    "            if not vem2_indices.empty:\n",
    "                vem1_index = find_threshold_crossing(df, vem2_indices[0], threshold, \"backward\", filename)\n",
    "                if vem1_index is not None:\n",
    "                    df.loc[vem1_index, 'VEM1'] = 1\n",
    "\n",
    "            # Find VEM4 (forward crossing) relative to VEM3\n",
    "            vem3_indices = df[df['VEM3'] == 1].index\n",
    "            if not vem3_indices.empty:\n",
    "                vem4_index = find_threshold_crossing(df, vem3_indices[0], threshold, \"forward\", filename)\n",
    "                if vem4_index is not None:\n",
    "                    df.loc[vem4_index, 'VEM4'] = 1\n",
    "\n",
    "            # Calculate Power and mark VEM5 at the maximum power point (between VEM1 and VEM2)\n",
    "            vem1_idx = df[df['VEM1'] == 1].index\n",
    "            vem2_idx = df[df['VEM2'] == 1].index\n",
    "            if not vem1_idx.empty and not vem2_idx.empty:\n",
    "                vem1_idx = vem1_idx[0]\n",
    "                vem2_idx = vem2_idx[0]\n",
    "                delta_t = df.loc[vem2_idx, 'Time'] - df.loc[vem1_idx, 'Time']\n",
    "                work = df['Total_GRF'] * df['PeakJumpHeight']\n",
    "                df.loc[vem1_idx:vem2_idx, 'Power'] = work[vem1_idx:vem2_idx] / delta_t\n",
    "                max_power_idx = df.loc[vem1_idx:vem2_idx, 'Power'].idxmax()\n",
    "                df.at[max_power_idx, 'VEM5'] = 1\n",
    "\n",
    "            # Save the processed file by adding a suffix before the file extension.\n",
    "            base_name, ext = os.path.splitext(filename)\n",
    "            output_file_name = f\"{base_name}_proc{ext}\"\n",
    "            output_file_path = os.path.join(output_folder, output_file_name)\n",
    "            df.to_excel(output_file_path, index=False, engine='openpyxl')\n",
    "            processed_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "            error_count += 1\n",
    "            continue\n",
    "\n",
    "    print(f\"\\nProcessing Complete:\")\n",
    "    print(f\"Files processed: {processed_count}\")\n",
    "    print(f\"Files with errors: {error_count}\")\n",
    "\n",
    "# -------- Function for Force Data Analysis --------\n",
    "def analyze_force_data(file_path):\n",
    "    \"\"\"\n",
    "    Reads a processed XLSX file and calculates the normalized GRF values\n",
    "    and COM velocity at the VEM5 event.\n",
    "    \"\"\"\n",
    "    # For final processed files, we assume a normal Excel file (no extra header rows)\n",
    "    df = pd.read_excel(file_path, engine='openpyxl')\n",
    "    \n",
    "    # Ensure required columns exist\n",
    "    required_columns = ['VEM1', 'VEM2', 'VEM3', 'VEM4', 'VEM5', 'Total_GRF', 'COM_Velocity']\n",
    "    for col in required_columns:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Column {col} not found in {file_path}\")\n",
    "    \n",
    "    # Assume the body weight is given by the absolute value of Total_GRF in the first row\n",
    "    body_weight = abs(df.loc[0, 'Total_GRF'])\n",
    "    \n",
    "    # Ensure that the necessary VEM markers are present\n",
    "    if df[df['VEM1'] == 1].empty or df[df['VEM2'] == 1].empty:\n",
    "        raise ValueError(f\"Missing VEM1 or VEM2 markers in {file_path}\")\n",
    "    if df[df['VEM3'] == 1].empty or df[df['VEM4'] == 1].empty:\n",
    "        raise ValueError(f\"Missing VEM3 or VEM4 markers in {file_path}\")\n",
    "    \n",
    "    # Determine phase indices\n",
    "    start_idx_1 = df[df['VEM1'] == 1].index[0]  # Phase 1 start\n",
    "    end_idx_1 = df[df['VEM2'] == 1].index[0]      # Phase 1 end\n",
    "    start_idx_2 = df[df['VEM3'] == 1].index[0]      # Phase 2 start\n",
    "    end_idx_2 = df[df['VEM4'] == 1].index[0]        # Phase 2 end\n",
    "\n",
    "    # Get the peak (most negative) Total_GRF during each phase\n",
    "    peak_grf_1 = df['Total_GRF'][start_idx_1:end_idx_1 + 1].max()\n",
    "    peak_grf_2 = df['Total_GRF'][start_idx_2:end_idx_2 + 1].max()\n",
    "\n",
    "    # Normalize the peak GRF values by body weight\n",
    "    normalized_peak_grf_1 = abs(peak_grf_1 / body_weight)\n",
    "    normalized_peak_grf_2 = abs(peak_grf_2 / body_weight)\n",
    "    avg_normalized_peak_force_1 = normalized_peak_grf_1  # (a single value)\n",
    "    percent_difference_1 = 0  # With a single value, this is set to zero\n",
    "    percent_difference_2 = 0\n",
    "\n",
    "    # Get GRF and COM velocity at VEM5\n",
    "    if df[df['VEM5'] == 1].empty:\n",
    "        raise ValueError(f\"Missing VEM5 marker in {file_path}\")\n",
    "    vem5_idx = df[df['VEM5'] == 1].index[0]\n",
    "    vem5_grf = abs(df.loc[vem5_idx, 'Total_GRF'])\n",
    "    vem5_com_velocity = df.loc[vem5_idx, 'COM_Velocity']\n",
    "    normalized_vem5_grf = abs(vem5_grf / body_weight)\n",
    "\n",
    "    return (avg_normalized_peak_force_1, normalized_peak_grf_1, normalized_peak_grf_2,\n",
    "            percent_difference_1, percent_difference_2, vem5_grf, vem5_com_velocity, normalized_vem5_grf)\n",
    "\n",
    "def run_analysis(processed_folder, report_file):\n",
    "    \"\"\"\n",
    "    Loops over all processed XLSX files, analyzes the force data,\n",
    "    and writes the combined results to an Excel report.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    # Loop through all XLSX files in the processed folder\n",
    "    processed_files = glob.glob(os.path.join(processed_folder, '*.xlsx'))\n",
    "    for file_path in processed_files:\n",
    "        try:\n",
    "            analysis_result = analyze_force_data(file_path)\n",
    "            # Extract participant initials from the filename (first 3 characters)\n",
    "            participant_initials = os.path.basename(file_path)[:3]\n",
    "            results.append([participant_initials] + list(analysis_result))\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {os.path.basename(file_path)}: {e}\")\n",
    "    \n",
    "    # Define column names for the report\n",
    "    results_df = pd.DataFrame(results, columns=[\n",
    "        'Participant', \n",
    "        'Avg Normalized Peak Force Phase 1', \n",
    "        'Normalized Peak GRF Phase 1', 'Normalized Peak GRF Phase 2', \n",
    "        'Percent Difference Phase 1 (%)', 'Percent Difference Phase 2 (%)',\n",
    "        'GRF at VEM5', 'COM Velocity at VEM5', 'Normalized GRF at VEM5'\n",
    "    ])\n",
    "    \n",
    "    # Ensure the directory for the report exists\n",
    "    os.makedirs(os.path.dirname(report_file), exist_ok=True)\n",
    "    \n",
    "    # Write the results to an Excel file with two sheets:\n",
    "    # one for individual trials and one for grouped (averaged) results\n",
    "    with pd.ExcelWriter(report_file, engine='openpyxl') as writer:\n",
    "        results_df.to_excel(writer, sheet_name='Individual Trials', index=False)\n",
    "        grouped_results = results_df.groupby('Participant').mean().reset_index()\n",
    "        grouped_results.to_excel(writer, sheet_name='Averaged by Participant', index=False)\n",
    "    \n",
    "    print(f\"\\nAnalysis report generated and saved to {report_file}\")\n",
    "\n",
    "# ----------------- Combined Pipeline Runner -----------------\n",
    "def run_pipeline():\n",
    "    print(\"Stage 1: Converting TXT files to XLSX...\")\n",
    "    batch_convert_txt_to_xlsx(TXT_INPUT_FOLDER, XLSX_INTERMEDIATE_FOLDER)\n",
    "    \n",
    "    print(\"\\nStage 2: Processing XLSX files...\")\n",
    "    process_files(XLSX_INTERMEDIATE_FOLDER, PROCESSED_XLSX_FOLDER)\n",
    "    \n",
    "    print(\"\\nStage 3: Running Force Data Analysis on Processed Files...\")\n",
    "    run_analysis(PROCESSED_XLSX_FOLDER, ANALYSIS_REPORT_FILE)\n",
    "    \n",
    "    print(\"\\nComplete pipeline finished!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_pipeline()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
